\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{p}{,}\PYG{l+m+mi}{28}\PYG{p}{))}
\PYG{n}{y\PYGZus{}target} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,],} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}target\PYGZus{}y\PYGZsq{}}\PYG{p}{)}
\PYG{n}{y\PYGZus{}onehot} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{to\PYGZus{}float}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{y\PYGZus{}target}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{),}
                                \PYG{n}{tf}\PYG{o}{.}\PYG{n}{to\PYGZus{}float}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant}\PYG{p}{(}
                                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)))))}
\PYG{n}{x\PYGZus{}reshape} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{o}{*}\PYG{l+m+mi}{28}\PYG{p}{])}
\PYG{n}{h} \PYG{o}{=} \PYG{n}{x\PYGZus{}reshape}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{):}
    \PYG{n}{h} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{(}\PYG{n}{linear}\PYG{p}{(}\PYG{n}{h}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{))}
\PYG{n}{yhat} \PYG{o}{=} \PYG{p}{(}\PYG{n}{linear}\PYG{p}{(}\PYG{n}{h}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{))}
\PYG{n}{crossEntropyError} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}
                \PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softmax\PYGZus{}cross\PYGZus{}entropy\PYGZus{}with\PYGZus{}logits}\PYG{p}{(}\PYG{n}{yhat}\PYG{p}{,} \PYG{n}{y\PYGZus{}onehot}\PYG{p}{))}
\PYG{n}{weightsError} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{add\PYGZus{}n}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}weights\PYGZus{}loss\PYGZdq{}}\PYG{p}{))}
\PYG{n}{loss} \PYG{o}{=} \PYG{n}{crossEntropyError} \PYG{o}{+} \PYG{l+m+mf}{3e\PYGZhy{}4}\PYG{o}{*}\PYG{n}{weightsError}
\PYG{n}{acc} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{to\PYGZus{}float}\PYG{p}{(}
                    \PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{to\PYGZus{}float}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{arg\PYGZus{}max}\PYG{p}{(}\PYG{n}{yhat}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)),}
                             \PYG{n}{tf}\PYG{o}{.}\PYG{n}{to\PYGZus{}float}\PYG{p}{(}\PYG{n}{y\PYGZus{}target}\PYG{p}{))))}
\PYG{n}{train\PYGZus{}op} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{AdamOptimizer}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,)}\PYG{o}{.}\PYG{n}{minimize}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{)}
\end{Verbatim}
